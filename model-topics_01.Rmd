---
title: "YouTubeR Topic Modeling"
output: html_notebook
---

```{r warning=FALSE}
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(require(magrittr))
suppressPackageStartupMessages(require(knitr))
suppressPackageStartupMessages(require(kableExtra))
"%nin%" = Negate("%in%")
```

```{r}
titles = read_rds(file = "./prepd-data/yt_titles-and-links.rds")
transcripts = read_rds(file = "./prepd-data/yt_transcripts.rds")
```

```{r echo=FALSE}
titles %>% 
  sample_n(10) %>% 
  kable(caption = "Preview: Title Dataset") %>% 
  kable_paper()
```
|- raw text 
|- remove new-line return symbols
|-- "\n" only
|- HTML special character normalization
|-- "&#39;" "&quot;" "&amp;" "&gt;" "&lt;"
|- Remove non-speach
|-- "[...]" "<...>"
|- flag transcript text with sentence-punctuation
|-- classification 
|- tokenize
|- index tokens
|- clean tokens
|- flag stopwords
|- stem tokens

```{r}
transcripts = transcripts %>% 
  # remove new-line markers
  mutate(text = str_replace_all(text, fixed("\n"), " ")) %>%
  # fix HTML characters
  mutate(text = str_replace_all(text, fixed("&#39;"), "'"),
         text = str_replace_all(text, fixed("&quot;"), "\""),
         text = str_replace_all(text, "&(amp;)+", " and "),
         text = str_replace_all(text, fixed("&gt"), ">"),
         text = str_replace_all(text, fixed("&lt;"), "<")) %>% 
  # remove inaudible
  mutate(text = str_replace_all(text, "\\[.*?\\]", " "),
         text = str_replace_all(text, "<.*?>", " ")
         ) %>% 
  mutate(sent_punct = str_detect(text, "[\\.\\?\\!]")) %>% 
  # remove empty transcript lines 
  mutate(text = trimws(text)) %>% 
  filter(text != "")

# some example out 
transcripts%>%
  filter(row_number() %in% c(224171,10970,35125,657437,345125,233379))
```

```{r}
# analyze sentence punctuation 
punct_summary = transcripts %>% 
  group_by(ref) %>% 
  summarise(dur = max(start, na.rm = TRUE) - min(start, na.rm = TRUE),
            puncts = sum(sent_punct)
            )
```

```{r warning=FALSE}
punct_summary %>% 
  ggplot(., aes(dur, puncts, color = puncts)) + 
  geom_density2d_filled(alpha = 0.2) +
  geom_point() + 
  scale_y_log10(labels = scales::number_format()) +
  scale_x_log10(labels = scales::number_format()) +
  geom_hline(yintercept = c(60,100), linetype = 2) + 
  labs(title = "Identifying Well Punctuated Transcripts",
       subtitle = "Good cut-off looks like its between 60 and 100 punctuated lines per video.",
       x = "Duration (s)", 
       y = "Punctuated Lines") + 
  theme(legend.position = "none")
```
```{r warning=FALSE}
clust = punct_summary %>%
  ungroup() %>%
  mutate(dur = dur + 1, 
          puncts = puncts + 1) %>% 
  select(dur, puncts) %>% 
  mutate_all(.tbl = ., .funs = log10) %>% 
  fpc::dbscan(data = ., eps = 0.65, MinPts = 200)
  
punct_summary %>% 
  mutate(k = factor(clust$cluster)) %>% 
  ggplot(., aes(dur, puncts)) + 
  geom_density2d_filled(alpha = 0.2) +
  geom_point(aes(color = k)) +
  scale_y_log10(labels = scales::number_format()) +
  # scale_x_log10(labels = scales::number_format()) +
  geom_hline(yintercept = c(30,100), linetype = 2) + 
  labs(title = "Identifying Well Punctuated Transcripts",
       subtitle = paste("Alternatively, dbscan can be used to segement populations.",
                        "Focusing on the 30 to 100 punctuated lines per video range.",
                        sep = "\n"),
       x = "Duration (s)", 
       y = "Punctuated Lines") +
  theme(legend.position = "none")

```
```{r}
# Which cluster is well punctuated, top cluster. 
clust.which =  punct_summary %>% 
  mutate(k = clust$cluster) %>% 
  group_by(k) %>% 
  summarise(dur_mean = mean(dur),
            punct_mean = mean(puncts)) %>% 
  arrange(-punct_mean)

clust.which
```

```{r}
# Classify well punctuated based on dbscan and hard-cut off @ 30 punct'd lines
punct_summary = punct_summary %>% # TODO save
  mutate(k = clust$cluster) %>% 
  mutate(.punctd = (puncts >= 30 & k == clust.which$k[1])) %>% 
  select(everything(), .punctd)

punct_summary %>% count(.punctd)
```

Since the minority of transcripts are well punctuated, we will keep all transcripts in the corpus; however, downstream snippet creation will only be available for those. 

```{r}
data = transcripts %>% # TODO save
  select(ref, text) %>%
  ## do not use, need to preserve punctuation 
  # tidytext::unnest_tokens(tbl = ., output = "word", to_lower = FALSE, input = text, token = "words") %>%
  mutate(word = str_split(text, "\\s+")) %>% 
  select(-text) %>% 
  unnest(data = ., cols = c(word)) %>% 
  # index words 
  group_by(ref) %>% 
  mutate(word_index = row_number()) %>% 
  ungroup() %>% 
  # pre-processing: lower case tokens w/o punct, stemmed
  mutate(token = str_remove_all(string = tolower(word), "[[:punct:]]"),
         token =  SnowballC::wordStem(words = token, language = "english")) %>% 
  # flag stop-words, excluded from model corpus
  mutate(stop_word = token %in% stopwords::data_stopwords_smart$en) %>% 
  mutate(stop_word = ifelse(str_detect(token, "^(h*)uh"), TRUE, stop_word),
         stop_word = ifelse(str_detect(token, "(m*)h(m+)"), TRUE, stop_word)) %>%
  mutate(stop_word = ifelse(str_detect(token, "\\d+"), TRUE, stop_word)) %>% 
  mutate(stop_word = ifelse(token == "", TRUE, stop_word)) 
```

```{r}
token_prevelence = data %>% 
  filter(!stop_word) %>%
  count(token)

.ecdf = ecdf(token_prevelence$n)
token_prevelence$ecdf = .ecdf(token_prevelence$n)
token_prevelence$high_prev = (token_prevelence$n >= 10)

token_prevelence %>% 
  ggplot(., aes(n)) +
  stat_ecdf() +
  scale_x_log10() +
  geom_vline(xintercept = 10, linetype = 2) +
  geom_hline(yintercept = .ecdf(10), linetype = 2)
```
```{r}
data = data %>% 
  mutate(high_prevance = token %in% filter(.data = token_prevelence, high_prev)$token)
```

```{r}
data %>% 
  filter(ref == "-2enQV3Dfng") %>% 
  filter(word_index %in% seq(1000,1200)) %$% 
  # head(200) %$%
  paste(word, collapse = " ") %>% 
  cat()
```
```{r}
data %>% 
  filter(ref == "-2enQV3Dfng") %>% 
  filter(word_index %in% seq(1000,1200)) %>% 
  filter(!stop_word & high_prevance) %$% 
  paste(token, collapse = " ") %>% 
  cat()
```
```{r echo=FALSE}
# Create sparse data struct for modeling
sparse_data = data %>% 
  filter(!stop_word, high_prevance) %>% 
  count(ref, token) %>% 
  tidytext::cast_sparse(data = ., row = ref, column = token, value = n)
```

```{r echo=FALSE}
# Model many models, wide k-interval
future::plan("multisession")
set.seed(123)
many_models = data_frame(K = seq(from = 10, to = 80, by = 10)) %>%
  mutate(topic_model = furrr::future_map(K, ~stm::stm(sparse_data, K = .,
                                          verbose = FALSE),
                                         .options = furrr::furrr_options(seed = TRUE), 
                                         .progress = TRUE)
         )
```

```{r}
# Calculate performance metrics
heldout = stm::make.heldout(sparse_data)
k_result = many_models %>%
  mutate(exclusivity = map(topic_model, stm::exclusivity),
         semantic_coherence = map(topic_model, stm::semanticCoherence, sparse_data),
         eval_heldout = map(topic_model, stm::eval.heldout, heldout$missing),
         residual = map(topic_model, stm::checkResiduals, sparse_data),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

```

```{r}
# Plot performance metrics
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1, alpha = 0.7, show.legend = FALSE) +
  geom_point(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")
```

```{r eval=FALSE, include=FALSE}
# Model many models, narrow k-interval
future::plan("multisession")
set.seed(123)
many_models = data_frame(K = seq(from = 15, to = 25, by = 5)) %>%
  mutate(topic_model = furrr::future_map(K, ~stm::stm(sparse_data, K = .,
                                          verbose = TRUE),
                                         .options = furrr::furrr_options(seed = TRUE), 
                                         .progress = TRUE)
         )
```

```{r eval=FALSE, include=FALSE}
# Calculate performance metrics
heldout = stm::make.heldout(sparse_data)
k_result = many_models %>%
  mutate(exclusivity = map(topic_model, stm::exclusivity),
         semantic_coherence = map(topic_model, stm::semanticCoherence, sparse_data),
         eval_heldout = map(topic_model, stm::eval.heldout, heldout$missing),
         residual = map(topic_model, stm::checkResiduals, sparse_data),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

```

```{r}
# Plot performance metrics
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")
```


```{r}
topic_model = many_models$topic_model[[2]] # TODO SAVE

theta = as_tibble(topic_model$theta)
names(theta) = paste0("topic_",seq(ncol(theta)))
theta$document = rownames(sparse_data)
theta = select(.data = theta, document, everything())

main_topic = theta %>% 
  pivot_longer(data = ., cols = contains("topic"), 
               names_to = "topic_id", values_to = "theta") %>% 
  group_by(document) %>% 
  slice_max(.data = ., order_by = theta, n =  1) %>% 
  left_join(x = .,
            y = select(.data = titles, ref, title),
            by = c("document" = "ref")) %>% 
  mutate(topic_id = factor(topic_id))
```

```{r}
save(list = c("titles",  "transcripts",
              "many_models",
              "punct_summary", "data"), 
     file = "./prepd-data/topic-model-data.RData"
     )
```

